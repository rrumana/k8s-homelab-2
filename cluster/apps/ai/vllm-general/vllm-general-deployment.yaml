apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-general
  labels:
    app: vllm-general
    app.kubernetes.io/component: model-worker
    app.kubernetes.io/part-of: vllm-workers
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: vllm-general
  template:
    metadata:
      labels:
        app: vllm-general
        app.kubernetes.io/component: model-worker
        app.kubernetes.io/part-of: vllm-workers
    spec:
      terminationGracePeriodSeconds: 120
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: llm/gpu
                    operator: In
                    values:
                      - "true"
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/part-of: vllm-workers
              topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app.kubernetes.io/part-of: vllm-workers
      initContainers:
        - name: fetch-gguf
          image: vllm/vllm-openai-rocm:latest
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
          args:
            - |
              set -eu
              python3 - <<'PY'
              import os
              import pathlib
              from huggingface_hub import hf_hub_download, list_repo_files

              repo = os.environ["VLLM_MODEL_REPO"]
              quant = os.environ["VLLM_GGUF_QUANT"].lower()
              token = os.getenv("HUGGING_FACE_HUB_TOKEN")

              files = list_repo_files(repo, token=token)
              candidates = [f for f in files if f.lower().endswith(".gguf") and quant in f.lower()]
              if not candidates:
                  raise RuntimeError(f"No GGUF file matching quant '{quant}' in repo '{repo}'")

              chosen = sorted(candidates, key=lambda f: (len(f), f.lower()))[0]
              source = hf_hub_download(
                  repo_id=repo,
                  filename=chosen,
                  token=token,
                  cache_dir="/models/hf-cache",
              )

              target_dir = pathlib.Path("/models/weights")
              target_dir.mkdir(parents=True, exist_ok=True)
              target = target_dir / "model.gguf"
              if target.exists() or target.is_symlink():
                  target.unlink()
              target.symlink_to(pathlib.Path(source))
              print(f"Using GGUF: {repo}/{chosen}")
              print(f"Linked model path: {target} -> {source}")
              PY
          envFrom:
            - configMapRef:
                name: vllm-general-config
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-shared-secret
                  key: HUGGING_FACE_HUB_TOKEN
                  optional: true
          volumeMounts:
            - name: model-cache
              mountPath: /models
      containers:
        - name: vllm
          image: vllm/vllm-openai-rocm:latest
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8000
              name: http
          envFrom:
            - configMapRef:
                name: vllm-general-config
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-shared-secret
                  key: HUGGING_FACE_HUB_TOKEN
                  optional: true
            - name: HF_HOME
              value: /models
          args:
            - $(VLLM_MODEL_PATH)
            - --load-format=gguf
            - --tokenizer=$(VLLM_TOKENIZER)
            - --hf-config-path=$(VLLM_HF_CONFIG_PATH)
            - --served-model-name=$(VLLM_SERVED_MODEL_NAME)
            - --download-dir=$(VLLM_DOWNLOAD_DIR)
            - --host=0.0.0.0
            - --port=$(VLLM_PORT)
            - --gpu-memory-utilization=$(VLLM_GPU_MEMORY_UTILIZATION)
            - --max-model-len=$(VLLM_MAX_MODEL_LEN)
            - --max-num-seqs=$(VLLM_MAX_NUM_SEQS)
            - --tensor-parallel-size=1
            - --enable-prefix-caching
            - --enable-chunked-prefill
            - --disable-log-requests
          volumeMounts:
            - name: model-cache
              mountPath: /models
          resources:
            requests:
              cpu: "4"
              memory: "16Gi"
              amd.com/gpu: "1"
            limits:
              cpu: "10"
              memory: "40Gi"
              amd.com/gpu: "1"
          startupProbe:
            httpGet:
              path: /health
              port: http
            failureThreshold: 180
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 20
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-general-cache
