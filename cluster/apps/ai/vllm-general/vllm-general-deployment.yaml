apiVersion: apps/v1
kind: Deployment
metadata:
  name: vllm-general
  labels:
    app: vllm-general
    app.kubernetes.io/component: model-worker
    app.kubernetes.io/part-of: vllm-workers
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: vllm-general
  template:
    metadata:
      labels:
        app: vllm-general
        app.kubernetes.io/component: model-worker
        app.kubernetes.io/part-of: vllm-workers
    spec:
      terminationGracePeriodSeconds: 120
      enableServiceLinks: false
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      affinity:
        nodeAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - weight: 100
              preference:
                matchExpressions:
                  - key: llm/gpu
                    operator: In
                    values:
                      - "true"
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchLabels:
                  app.kubernetes.io/part-of: vllm-workers
              topologyKey: kubernetes.io/hostname
      topologySpreadConstraints:
        - maxSkew: 1
          topologyKey: kubernetes.io/hostname
          whenUnsatisfiable: DoNotSchedule
          labelSelector:
            matchLabels:
              app.kubernetes.io/part-of: vllm-workers
      initContainers:
        - name: prepare-patched-model
          image: ghcr.io/rrumana/vllm-openai-rocm-gfx1150:nightly-gfx1150-63f1fe04d873
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef:
                name: vllm-general-config
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-shared-secret
                  key: HUGGING_FACE_HUB_TOKEN
                  optional: true
          command:
            - /bin/sh
            - -ec
          args:
            - |
              python3 - <<'PY'
              import json
              import os
              import subprocess
              import sys

              source_model = os.environ["SOURCE_MODEL_ID"]
              local_dir = os.environ["MODEL_ID"]
              token = os.getenv("HUGGING_FACE_HUB_TOKEN")

              try:
                  from huggingface_hub import snapshot_download
              except Exception:
                  subprocess.check_call(
                      [sys.executable, "-m", "pip", "install", "--no-cache-dir", "huggingface_hub"]
                  )
                  from huggingface_hub import snapshot_download

              try:
                  snapshot_download(
                      repo_id=source_model,
                      local_dir=local_dir,
                      token=token,
                      local_dir_use_symlinks=False,
                      resume_download=True,
                  )
              except TypeError:
                  snapshot_download(
                      repo_id=source_model,
                      local_dir=local_dir,
                      token=token,
                  )

              cfg_path = os.path.join(local_dir, "config.json")
              with open(cfg_path, "r", encoding="utf-8") as f:
                  cfg = json.load(f)

              qcfg = cfg.get("quantization_config") or {}
              groups = qcfg.get("config_groups") or {}
              injected = [
                  "re:.*fused_qkv_a_proj$",
                  "re:.*fused_qkv_b_proj$",
                  "re:.*fused_kv_a_proj_with_mqa$",
                  "re:.*fused_kv_b_proj$",
                  "re:.*fused_qkv.*",
                  "re:.*fused_kv.*",
              ]

              changed = False
              for _, group in groups.items():
                  targets = group.get("targets") or []
                  for target in injected:
                      if target not in targets:
                          targets.append(target)
                          changed = True
                  group["targets"] = targets

              if changed:
                  with open(cfg_path, "w", encoding="utf-8") as f:
                      json.dump(cfg, f, ensure_ascii=True, indent=2)
                      f.write("\n")
                  print("Patched quantization targets in", cfg_path)
              else:
                  print("Quantization targets already patched in", cfg_path)
              PY
          volumeMounts:
            - name: model-cache
              mountPath: /models
        - name: install-transformers-nightly
          image: ghcr.io/rrumana/vllm-openai-rocm-gfx1150:nightly-gfx1150-63f1fe04d873
          imagePullPolicy: IfNotPresent
          command:
            - /bin/sh
            - -c
          args:
            - >
              python3 -m pip install --no-cache-dir --upgrade
              --target /opt/pydeps
              git+https://github.com/huggingface/huggingface_hub.git
              &&
              python3 -m pip install --no-cache-dir --upgrade --no-deps
              --target /opt/pydeps
              git+https://github.com/huggingface/transformers.git
          volumeMounts:
            - name: pydeps
              mountPath: /opt/pydeps
      containers:
        - name: vllm
          image: ghcr.io/rrumana/vllm-openai-rocm-gfx1150:nightly-gfx1150-63f1fe04d873
          imagePullPolicy: IfNotPresent
          command:
            - vllm
            - serve
          ports:
            - containerPort: 8000
              name: http
          envFrom:
            - configMapRef:
                name: vllm-general-config
          env:
            - name: HUGGING_FACE_HUB_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-shared-secret
                  key: HUGGING_FACE_HUB_TOKEN
                  optional: true
            - name: HSA_OVERRIDE_GFX_VERSION
              value: "11.5.0"
            - name: HF_HOME
              value: /models
            - name: PYTHONPATH
              value: /opt/pydeps
          args:
            - $(MODEL_ID)
            - --tokenizer=$(TOKENIZER_ID)
            - --dtype=float16
            - --tokenizer-mode=auto
            - --enable-auto-tool-choice
            - --tool-call-parser=glm47
            - --reasoning-parser=glm45
            - --served-model-name=$(SERVED_MODEL_NAME)
            - --download-dir=$(DOWNLOAD_DIR)
            - --host=0.0.0.0
            - --port=$(PORT)
            - --gpu-memory-utilization=$(GPU_MEMORY_UTILIZATION)
            - --max-model-len=$(MAX_MODEL_LEN)
            - --max-num-seqs=$(MAX_NUM_SEQS)
            - --tensor-parallel-size=1
            - --enable-prefix-caching
            - --enforce-eager
            - --disable-log-requests
          volumeMounts:
            - name: model-cache
              mountPath: /models
            - name: pydeps
              mountPath: /opt/pydeps
          resources:
            requests:
              cpu: "4"
              memory: "16Gi"
              amd.com/gpu: "1"
            limits:
              cpu: "10"
              memory: "40Gi"
              amd.com/gpu: "1"
          startupProbe:
            httpGet:
              path: /health
              port: http
            failureThreshold: 180
            periodSeconds: 10
          readinessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            httpGet:
              path: /health
              port: http
            initialDelaySeconds: 30
            periodSeconds: 20
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: vllm-general-cache
        - name: pydeps
          emptyDir: {}
