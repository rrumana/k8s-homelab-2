# Q4 2025 Cluster Update

I’ve decided that I’m going to give quarterly updates on the Kubernetes Cluster since now I have users and people might care what I do to it. Most will probably not be this long but a lot happened ngl. Also whenever you’d like we can schedule a time and I’ll do a small one on one orientation where I can walk you through all the services, get you connected with your accounts, and get you started.

Since our last Discord call a ton has changed. I actually built a new cluster from the ground up with the hindsight I gleaned from the first one. Much of the deployments are the same, but the platform code is nearly all different. As such the Repo has moved from k8s-homeland to k8s-homelab-2. As for why I didn’t just make a V2 branch, I was thinking of developing this new cluster over a much longer period of time, and then busted it out in a week. The old cluster has been retired and is no longer in service. Downtime was approximately 1 hour in the middle of the night for Tailscale users and approximately 36 hours for the general public, which I found acceptable since I’m not currently seeking employment.

---

## Cluster Changelog

### Hardware Changes
Added 3rd node
- Melchior-1
- Balthasar-2
- Casper-3

Updated aggregate resources:
- 36 CPU cores
- 192GB addressable RAM
- 4TB of cluster storage (CEPH)
- 11TB of NAS storage (ZFS)

### Platform Changes
- Switch to HA control plane (etcd)
- Replace Flannel with Cilium CNI
- Add Linkerd service mesh
- Migrate from longhorn to Rook/Ceph
- Added dedicated 2.5Gb storage network
- Added Desheduler
- Remove worker nodes from Tailnet
- Nextcloud database migration to Postgres
- Migrate Immich Postgres to PVC
- Migrate Immich Models to PVC

### Service Changes
- Added Headscale server
- Added jellyseer to arr stack
- Added email server to cluster
- immich-admin@rcrumana.xyz
- nextcloud-admin@rcrumana.xyz
- Re-added Plex port forward
- Remove Llama.cpp
- Remove Qdrant db
- Remove Rancher
- Remove Longhorn
- Remove n8n

### Quality of life Changes
- Added torrenting rate limit of 80%
- Redid Nextcloud accounts
- Admin
- Ryan + Kaylee
- Ethan + Ashley
- Mitch + Sarah
- Redid Immich accounts
- Admin
- Ryan + Kaylee account
- Ethan + Ashley account
- Mitch + Sarah account
- Jellyfin account management
- Admin account
- Mitch and Sarah
- Ethan and Ashley
- Ryan and Kaylee
- Homarr account management
- Rcrumana -> Ryan + Kaylee
- Added Mitch + Sarah
- Added Ethan + Ashley
- Made “User” board

### Bug fixes
- Clear All Nextcloud errors
- Fixed node NAS backups

---

## To Do

### Hardware Changes
- Lower addressable RAM
- 192GB -> 144GB
- Raise GPU RAM
- 96GB -> 144GB

### Platform Changes
- Put Kube API behind load balancer
- Update node IPs
- Add clustered compute engine for vLLM
- Add Postgres Operator
- Add Redis Operator
- Add Loki
- Add Graphana
- Add Promtail
- Enforce HTTP/2 internally
- Enable HTTP/3 externally (Cloudflare)
- Switch services to HA Postgres
- Radarr
- Sonarr
- Lidarr,
- Prowlarr
- Jellyseer
- Homarr
- Uptime Kuma
- Vaultwarden

### Service Changes
- Set up Jellyseer
- Integrate with Plex + Jellyfin
- Add to User dashboard
- Add vLLM service

### Quality of life Changes
- None Planned

### Bug fixes
- Linkerd flaky on Balthasar-2
- Likely OS permission issue
- Going to require node downtime
- Immich facial recognition failing
- Intermittent failure
- No idea
- Please submit bug reports if you have issues

---

## Conclusion

As you can see, somehow I ended up having a full rebuild sprint in between opening the cluster to y’all and actually onboarding y’all but it was also fun so don’t worry about it.

Don’t always expect this level of active development, I want to get this iteration of the cluster to a healthy point where it’s ready for the storage and traffic of more users, as well as ready for the deployment of services like websites, small servers, and whatnot. I even use the cluster for my containerization pipeline so every time I git push to the main branch on my website the cluster makes and deploys a new docker container so I don’t have to fiddle with the redeploy. Makes website deployments great cause I can test locally, git push, wait a minute, and then check the real site to make sure the changes are live.

You might have noticed we have a TON of RAM, I got really lucky and scored some DDR5 SODIMMs super cheap before RAM prices skyrocketed. Going to use it for some distributed LLM experiments. If they go well I’ll integrate them into the local LLM setup, otherwise available models will be capped at 40GB for MOE models and 16GB for dense models.

Let me know if there are services you want to add, websites you want to deploy, or things you want to try, once I get the cluster a little more stable and add some more ease of use features spinning up new services with TLS certs, CI/CD pipelines, Blue/Green deployments, staging environments, logging/dashboards, and so on will take minutes instead of days.

That said, I just landed in Denver and I’m not doing any work on it for the time being. I can always SSH in and fix big problems if they happen, but no development till mid January.

Have a Merry Christmas Nerds
