apiVersion: apps/v1
kind: Deployment
metadata:
  name: llama-static-b
  labels:
    app: llama-static-b
    app.kubernetes.io/component: model-worker
    app.kubernetes.io/part-of: llama-backend
spec:
  replicas: 1
  revisionHistoryLimit: 2
  selector:
    matchLabels:
      app: llama-static-b
  template:
    metadata:
      labels:
        app: llama-static-b
        app.kubernetes.io/component: model-worker
        app.kubernetes.io/part-of: llama-backend
    spec:
      terminationGracePeriodSeconds: 120
      enableServiceLinks: false
      tolerations:
        - key: node-role.kubernetes.io/control-plane
          operator: Exists
          effect: NoSchedule
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
            - labelSelector:
                matchExpressions:
                  - key: app.kubernetes.io/component
                    operator: In
                    values:
                      - model-worker
                  - key: app.kubernetes.io/part-of
                    operator: In
                    values:
                      - llama-backend
              topologyKey: kubernetes.io/hostname
      initContainers:
        - name: fetch-gguf
          image: python:3.12-slim
          imagePullPolicy: IfNotPresent
          envFrom:
            - configMapRef:
                name: llama-static-b-config
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-shared-secret
                  key: HUGGING_FACE_HUB_TOKEN
                  optional: true
            - name: HF_HOME
              value: /models/hf-cache/static-b
          command:
            - /bin/sh
            - -ec
          args:
            - |
              python3 -m pip install --no-cache-dir huggingface_hub
              python3 - <<'PY'
              import os
              import pathlib
              from huggingface_hub import hf_hub_download, list_repo_files

              repo = os.environ["MODEL_REPO"]
              quant = os.environ.get("MODEL_QUANT", "").strip().lower()
              filename = os.environ.get("MODEL_FILENAME", "").strip()
              token = os.getenv("HF_TOKEN")

              if filename:
                  chosen = filename
              else:
                  files = list_repo_files(repo_id=repo, token=token)
                  candidates = [f for f in files if f.lower().endswith(".gguf")]
                  if quant:
                      candidates = [f for f in candidates if quant in f.lower()]
                  if not candidates:
                      raise RuntimeError(
                          f"No GGUF file found for repo '{repo}' with quant filter '{quant}'"
                      )
                  chosen = sorted(candidates, key=lambda f: (len(f), f.lower()))[0]

              source = hf_hub_download(
                  repo_id=repo,
                  filename=chosen,
                  token=token,
                  cache_dir="/models/hf-cache",
              )
              target = pathlib.Path(os.environ["MODEL_PATH"])
              target.parent.mkdir(parents=True, exist_ok=True)
              if target.exists() or target.is_symlink():
                  target.unlink()
              target.symlink_to(pathlib.Path(source))

              print(f"Using GGUF: {repo}/{chosen}")
              print(f"Linked model path: {target} -> {source}")
              PY
          volumeMounts:
            - name: model-cache
              mountPath: /models
      containers:
        - name: llama-server
          image: rocm/llama.cpp:llama.cpp-b6652.amd0_rocm7.0.0_ubuntu24.04_server
          imagePullPolicy: IfNotPresent
          command:
            - /app/llama-server
          ports:
            - containerPort: 8000
              name: http
          envFrom:
            - configMapRef:
                name: llama-static-b-config
          env:
            - name: HF_TOKEN
              valueFrom:
                secretKeyRef:
                  name: vllm-shared-secret
                  key: HUGGING_FACE_HUB_TOKEN
                  optional: true
            - name: HF_HOME
              value: /models/hf-cache/static-b
            - name: HSA_OVERRIDE_GFX_VERSION
              value: "11.5.1"
          args:
            - --model
            - $(MODEL_PATH)
            - --host
            - 0.0.0.0
            - --port
            - $(PORT)
            - --ctx-size
            - $(N_CTX)
            - --parallel
            - $(N_PARALLEL)
            - --threads
            - $(N_THREADS)
            - --n-gpu-layers
            - $(N_GPU_LAYERS)
            - --jinja
          volumeMounts:
            - name: model-cache
              mountPath: /models
          resources:
            requests:
              cpu: "4"
              memory: "16Gi"
              amd.com/gpu: "1"
            limits:
              cpu: "10"
              memory: "40Gi"
              amd.com/gpu: "1"
          startupProbe:
            tcpSocket:
              port: http
            failureThreshold: 180
            periodSeconds: 10
          readinessProbe:
            tcpSocket:
              port: http
            initialDelaySeconds: 10
            periodSeconds: 10
          livenessProbe:
            tcpSocket:
              port: http
            initialDelaySeconds: 30
            periodSeconds: 20
      volumes:
        - name: model-cache
          persistentVolumeClaim:
            claimName: llama-models-cache
