apiVersion: v1
kind: ConfigMap
metadata:
  name: llama-swap-config
data:
  SWAP_PORT: "8000"
  SWAP_MODELS: |
    GLM-4.7-Flash|unsloth/GLM-4.7-Flash-GGUF|Q4_K_M
    gpt-oss-20b|unsloth/gpt-oss-20b-GGUF|Q4_K_M
    gemma-3-12b-it|unsloth/gemma-3-12b-it-GGUF|Q4_K_M
    Qwen3-Coder-Next|unsloth/Qwen3-Coder-Next-GGUF|Q3_K_M
  config.yaml: |
    healthCheckTimeout: 3600
    models:
      GLM-4.7-Flash:
        cmd: >
          /app/llama-server
          --model /models/swap/GLM-4.7-Flash/model.gguf
          --ctx-size 262144
          --n-gpu-layers 999
          --parallel 4
          --threads 8
          --jinja
          --host 0.0.0.0
          --port ${PORT}
        ttl: 1800
      gpt-oss-20b:
        cmd: >
          /app/llama-server
          --model /models/swap/gpt-oss-20b/model.gguf
          --ctx-size 262144
          --n-gpu-layers 999
          --parallel 4
          --threads 8
          --jinja
          --host 0.0.0.0
          --port ${PORT}
        ttl: 1800
      gemma-3-12b-it:
        cmd: >
          /app/llama-server
          --model /models/swap/gemma-3-12b-it/model.gguf
          --ctx-size 131072
          --n-gpu-layers 999
          --parallel 4
          --threads 8
          --jinja
          --host 0.0.0.0
          --port ${PORT}
        ttl: 1800
      Qwen3-Coder-Next:
        cmd: >
          /app/llama-server
          --model /models/swap/Qwen3-Coder-Next/model.gguf
          --ctx-size 131072
          --n-gpu-layers 999
          --parallel 4
          --threads 8
          --jinja
          --host 0.0.0.0
          --port ${PORT}
        ttl: 1800
